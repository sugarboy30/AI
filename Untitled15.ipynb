{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1dS_xVi_B8UzwhsJ-SCkC3qepuX6a5unt",
      "authorship_tag": "ABX9TyPt6sA/OLGTJV6Ryhoq+lYb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarboy30/IAI/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-bL2tqTKqQYJ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers, Sequential"
      ],
      "metadata": {
        "id": "9f6OPtnpqRzX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3yOV07g0q44f",
        "outputId": "ca5fe0d7-a600-497e-c014-0260a07ad806"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(Model):\n",
        "    def __init__(self, c_in, c_out, is_downsample=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self._is_downsample = is_downsample\n",
        "\n",
        "        # block1\n",
        "        self._conv1 = layers.Conv2D(filters=c_out,\n",
        "                                    kernel_size=3,\n",
        "                                    strides=2 if is_downsample else 1,\n",
        "                                    padding=\"same\",\n",
        "                                    use_bias=False)\n",
        "        self._bn1 = layers.BatchNormalization()\n",
        "        self._relu1 = layers.ReLU()\n",
        "\n",
        "        # block2\n",
        "        self._conv2 = layers.Conv2D(filters=c_out,\n",
        "                                    kernel_size=3,\n",
        "                                    strides=1,\n",
        "                                    padding=\"same\",\n",
        "                                    use_bias=False)\n",
        "        self._bn2 = layers.BatchNormalization()\n",
        "        self._relu2 = layers.ReLU()\n",
        "        self._add = layers.Add()\n",
        "\n",
        "        # block3\n",
        "        self._downsample = None\n",
        "        if is_downsample:\n",
        "            self._downsample_conv1 = layers.Conv2D(filters=c_out,\n",
        "                                                   kernel_size=1,\n",
        "                                                   strides=2,\n",
        "                                                   padding=\"same\",\n",
        "                                                   use_bias=False)\n",
        "\n",
        "            self._downsample_bn1 = layers.BatchNormalization()\n",
        "\n",
        "        elif c_in != c_out:\n",
        "            self._downsample_conv1 = layers.Conv2D(filters=c_out,\n",
        "                                                   kernel_size=1,\n",
        "                                                   strides=1,\n",
        "                                                   padding=\"same\",\n",
        "                                                   use_bias=False)\n",
        "\n",
        "            self._downsample_bn1 = layers.BatchNormalization()\n",
        "            self._is_downsample = True\n",
        "\n",
        "    def call(self, x):\n",
        "        y = self._conv1(x)\n",
        "        y = self._bn1(y)\n",
        "        y = self._relu1(y)\n",
        "        y = self._conv2(y)\n",
        "        y = self._bn2(y)\n",
        "\n",
        "        if self._is_downsample:\n",
        "            x = self._downsample_conv1(x)\n",
        "            x = self._downsample_bn1(x)\n",
        "\n",
        "        y = self._relu2(self._add([x, y]))\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "def make_layers(c_in, c_out, repeat_times, is_downsample=False):\n",
        "    model = Sequential()\n",
        "    model.add(BasicBlock(c_in, c_out, is_downsample=is_downsample))\n",
        "    for _ in range(repeat_times - 1):\n",
        "        model.add(BasicBlock(c_out, c_out))\n",
        "    return model"
      ],
      "metadata": {
        "id": "z6sNzrhSq7jG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(Model):\n",
        "    def __init__(self, num_classes=2811, reid=False):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self._conv = Sequential([\n",
        "            layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
        "                          padding=\"same\"),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ELU(),\n",
        "            layers.Conv2D(\n",
        "                32,\n",
        "                3,\n",
        "                1,\n",
        "                padding=\"same\",\n",
        "            ),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ELU(),\n",
        "            layers.MaxPool2D(3, 2, padding=\"same\")\n",
        "        ])\n",
        "\n",
        "        self._layer1 = make_layers(32, 32, 2, False)\n",
        "        self._layer2 = make_layers(32, 64, 2, True)\n",
        "        self._layer3 = make_layers(64, 128, 2, True)\n",
        "\n",
        "        self._flatten = layers.Flatten()\n",
        "        self._dense_drop = layers.Dropout(0.6)\n",
        "        self._dense = layers.Dense(128)\n",
        "\n",
        "        if reid:\n",
        "            self._norm = layers.experimental.preprocessing.Normalization()\n",
        "\n",
        "        else:\n",
        "            self._dense_bn = layers.BatchNormalization()\n",
        "            self._dense_elu = layers.ELU()\n",
        "            self._classifier = layers.Dense(num_classes)\n",
        "\n",
        "        self._reid = reid\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self._conv(x)\n",
        "        x = self._layer1(x)\n",
        "        x = self._layer2(x)\n",
        "        x = self._layer3(x)\n",
        "\n",
        "        x = self._flatten(x)\n",
        "        x = self._dense_drop(x)\n",
        "        x = self._dense(x)\n",
        "\n",
        "        if self._reid:\n",
        "            # x = tf.math.l2_normalize(x)\n",
        "            x /= self._norm(x)\n",
        "            return x\n",
        "\n",
        "        x = self._dense_bn(x)\n",
        "        x = self._dense_elu(x)\n",
        "        x = self._classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Hcyc9bVArEJe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwYEdiAi_ajR",
        "outputId": "0f3175ac-2695-4e4b-cbcc-56e64c2adae5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Preprocessing functions\n",
        "def val_preprocess(img, label):\n",
        "    img = img / 255.0\n",
        "    return img, label\n",
        "\n",
        "def train_preprocess(img, label):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = img / 255.0\n",
        "    return img, label\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(train_dir, label_dir):\n",
        "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        batch_size=64,\n",
        "        image_size=(128, 64),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        validation_split=0.2,\n",
        "        subset=\"training\",\n",
        "        interpolation=\"bilinear\"\n",
        "    )\n",
        "\n",
        "    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        batch_size=64,\n",
        "        image_size=(128, 64),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        validation_split=0.2,\n",
        "        subset=\"validation\",\n",
        "        interpolation=\"bilinear\"\n",
        "    )\n",
        "\n",
        "    train_dataset = train_dataset.map(train_preprocess)\n",
        "    val_dataset = val_dataset.map(val_preprocess)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Model definition (replace Net() with your model definition)\n",
        "class Net(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define your model layers here\n",
        "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')\n",
        "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "def train():\n",
        "    train_ds, val_ds = load_dataset(\"/content/drive/MyDrive/1-28feb_ForkLift.v1i.yolov7pytorch/train\", \"/content/drive/MyDrive/1-28feb_ForkLift.v1i.yolov7pytorch/valid\")\n",
        "\n",
        "    # Split validation dataset\n",
        "    val_batches = tf.data.experimental.cardinality(val_ds).numpy()\n",
        "    test_ds = val_ds.take(val_batches // 5)\n",
        "    val_ds = val_ds.skip(val_batches // 5)\n",
        "\n",
        "    # Use buffered prefetching to load images from disk without having I/O become blocking\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def scheduler(epoch, lr):\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            lr *= 0.1\n",
        "        tf.summary.scalar('learning rate', data=lr, step=epoch)\n",
        "        return lr\n",
        "\n",
        "    checkpoint_filepath = \"./deepsort_checkpoint\"\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                           save_weights_only=True,\n",
        "                                           monitor='val_accuracy',\n",
        "                                           mode='max',\n",
        "                                           save_best_only=True),\n",
        "        tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    ]\n",
        "\n",
        "    model = Net()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        epochs=10,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=val_ds,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Vn4fwJnc_eMh",
        "outputId": "3cfe67ab-bca2-493b-9d23-a64f622cf63f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3108 files belonging to 2 classes.\n",
            "Using 2487 files for training.\n",
            "Found 3108 files belonging to 2 classes.\n",
            "Using 621 files for validation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-85ddfcdb598f>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-85ddfcdb598f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimizer must have a \"lr\" attribute.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# new API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m             \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2275\u001b[0m             \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Support for old API for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   4214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_in_graph_mode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4218\u001b[0m         \u001b[0;31m# This is a variable which was created in an eager context, but is being\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    691\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \"\"\"\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[1;32m    809\u001b[0m           self.handle, self._dtype)\n\u001b[1;32m    810\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    532\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    535\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Preprocessing functions\n",
        "def val_preprocess(img, label):\n",
        "    img = img / 255.0\n",
        "    return img, label\n",
        "\n",
        "def train_preprocess(img, label):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = img / 255.0\n",
        "    return img, label\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(train_dir):\n",
        "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        batch_size=64,\n",
        "        image_size=(128, 64),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        validation_split=0.2,\n",
        "        subset=\"training\",\n",
        "        interpolation=\"bilinear\"\n",
        "    )\n",
        "\n",
        "    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        batch_size=64,\n",
        "        image_size=(128, 64),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        validation_split=0.2,\n",
        "        subset=\"validation\",\n",
        "        interpolation=\"bilinear\"\n",
        "    )\n",
        "\n",
        "    train_dataset = train_dataset.map(train_preprocess)\n",
        "    val_dataset = val_dataset.map(val_preprocess)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Model definition\n",
        "class Net(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')\n",
        "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "def train():\n",
        "    train_ds, val_ds = load_dataset(\"/content/drive/MyDrive/1-28feb_ForkLift.v1i.yolov7pytorch/train\")\n",
        "\n",
        "    # Split validation dataset\n",
        "    val_batches = tf.data.experimental.cardinality(val_ds).numpy()\n",
        "    test_ds = val_ds.take(val_batches // 5)\n",
        "    val_ds = val_ds.skip(val_batches // 5)\n",
        "\n",
        "    # Use buffered prefetching\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def scheduler(epoch, lr):\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            lr *= 0.1\n",
        "        tf.summary.scalar('learning rate', data=lr, step=epoch)\n",
        "        return lr\n",
        "\n",
        "    checkpoint_filepath = \"./deepsort_checkpoint.weights.h5\"\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                           save_weights_only=True,\n",
        "                                           monitor='val_accuracy',\n",
        "                                           mode='max',\n",
        "                                           save_best_only=True),\n",
        "        tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    ]\n",
        "\n",
        "    model = Net()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        epochs=60,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=val_ds,\n",
        "    )\n",
        "\n",
        "    # Save model in SavedModel format\n",
        "    model.save(\"deepsort_saved_model\", save_format=\"tf\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9ByI5eNfWTh",
        "outputId": "043afca9-c2ac-4d7e-f508-b9df2493d249"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3108 files belonging to 2 classes.\n",
            "Using 2487 files for training.\n",
            "Found 3108 files belonging to 2 classes.\n",
            "Using 621 files for validation.\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 1/60\n",
            "39/39 [==============================] - 46s 258ms/step - loss: 0.0595 - accuracy: 0.9743 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 2/60\n",
            "39/39 [==============================] - 13s 264ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 3/60\n",
            "39/39 [==============================] - 14s 313ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 4/60\n",
            "39/39 [==============================] - 14s 303ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 5/60\n",
            "39/39 [==============================] - 14s 304ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 6/60\n",
            "39/39 [==============================] - 14s 302ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 7/60\n",
            "39/39 [==============================] - 14s 309ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 8/60\n",
            "39/39 [==============================] - 14s 311ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 9/60\n",
            "39/39 [==============================] - 13s 264ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 10/60\n",
            "39/39 [==============================] - 13s 299ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 11/60\n",
            "39/39 [==============================] - 13s 300ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 12/60\n",
            "39/39 [==============================] - 14s 311ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 13/60\n",
            "39/39 [==============================] - 14s 302ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 14/60\n",
            "39/39 [==============================] - 14s 301ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 15/60\n",
            "39/39 [==============================] - 16s 330ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 16/60\n",
            "39/39 [==============================] - 13s 295ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 17/60\n",
            "39/39 [==============================] - 14s 305ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 18/60\n",
            "39/39 [==============================] - 14s 307ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
            "Epoch 19/60\n",
            "39/39 [==============================] - 13s 298ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.1000\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.010000000149011612.\n",
            "Epoch 20/60\n",
            "39/39 [==============================] - 14s 305ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 21/60\n",
            "39/39 [==============================] - 13s 301ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 22/60\n",
            "39/39 [==============================] - 14s 308ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 23/60\n",
            "39/39 [==============================] - 13s 298ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 24/60\n",
            "39/39 [==============================] - 13s 299ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 25/60\n",
            "39/39 [==============================] - 13s 297ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 26/60\n",
            "39/39 [==============================] - 14s 310ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 27/60\n",
            "39/39 [==============================] - 14s 310ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 28/60\n",
            "39/39 [==============================] - 13s 299ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 29/60\n",
            "39/39 [==============================] - 13s 295ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 30/60\n",
            "39/39 [==============================] - 14s 301ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 31/60\n",
            "39/39 [==============================] - 13s 298ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 32/60\n",
            "39/39 [==============================] - 13s 296ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 33/60\n",
            "39/39 [==============================] - 14s 265ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 34/60\n",
            "39/39 [==============================] - 14s 267ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 35/60\n",
            "39/39 [==============================] - 13s 258ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 36/60\n",
            "39/39 [==============================] - 14s 275ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 37/60\n",
            "39/39 [==============================] - 14s 279ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 38/60\n",
            "39/39 [==============================] - 14s 270ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.009999999776482582.\n",
            "Epoch 39/60\n",
            "39/39 [==============================] - 13s 299ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0100\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.0009999999776482583.\n",
            "Epoch 40/60\n",
            "39/39 [==============================] - 14s 305ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 41/60\n",
            "39/39 [==============================] - 13s 296ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 42/60\n",
            "39/39 [==============================] - 16s 333ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 43/60\n",
            "39/39 [==============================] - 13s 299ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 44/60\n",
            "39/39 [==============================] - 14s 327ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 45/60\n",
            "39/39 [==============================] - 15s 340ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 46/60\n",
            "39/39 [==============================] - 13s 298ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 47/60\n",
            "39/39 [==============================] - 13s 272ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 48/60\n",
            "39/39 [==============================] - 14s 292ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 49/60\n",
            "39/39 [==============================] - 13s 287ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 50/60\n",
            "39/39 [==============================] - 13s 282ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 51/60\n",
            "39/39 [==============================] - 13s 292ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 52/60\n",
            "39/39 [==============================] - 14s 301ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 53/60\n",
            "39/39 [==============================] - 13s 295ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 54/60\n",
            "39/39 [==============================] - 15s 344ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 55/60\n",
            "39/39 [==============================] - 13s 295ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 56/60\n",
            "39/39 [==============================] - 13s 297ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 57/60\n",
            "39/39 [==============================] - 16s 356ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 58/60\n",
            "39/39 [==============================] - 13s 298ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.0009999999310821295.\n",
            "Epoch 59/60\n",
            "39/39 [==============================] - 14s 304ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 9.999999310821295e-05.\n",
            "Epoch 60/60\n",
            "39/39 [==============================] - 13s 298ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\n",
        "    \"deepsort.h5\", overwrite=True, save_format=\"h5\"\n",
        ")"
      ],
      "metadata": {
        "id": "eXwbLqZddQap"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}